# üåç Language Translation using Llama ü¶ô
This repository showcases a Language Translation model built using the Llama architecture. The model is trained to translate between multiple languages, leveraging datasets from Hugging Face and fine-tuned using LoRa (Low-Rank Adaptation) for enhanced performance.

## Features:
1. Multilingual Translation: Translates between various languages efficiently.
2. Llama Model: Powered by the state-of-the-art Llama architecture for language modeling.
3. LoRa Adaptation: Low-Rank Adaptation for faster and efficient fine-tuning.


## Project Structure:
notebooks/: Contains the core Jupyter notebooks with the translation model and experiments.

## Highlights:
1. Efficient model fine-tuning using LoRa.
2. Clean implementation using Hugging Face datasets.
   
## Tech Stack:
1. Python (Jupyter Notebooks)
2. Llama Model (Language Modeling)
3. LoRa (Low-Rank Adaptation)
4. Hugging Face Datasets
